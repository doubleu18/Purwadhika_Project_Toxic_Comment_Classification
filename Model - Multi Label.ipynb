{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('trainMaster.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments per Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sns.set(font_scale = 2)\n",
    "# plt.figure(figsize=(15,8))\n",
    "# ax= sns.barplot(categories, data_raw.iloc[:,2:].sum().values)\n",
    "# plt.title(\"Comments in each category\", fontsize=24)\n",
    "# plt.ylabel('Number of comments', fontsize=18)\n",
    "# plt.xlabel('Comment Type ', fontsize=18)\n",
    "# #adding the text labels\n",
    "# rects = ax.patches\n",
    "# labels = data_raw.iloc[:,2:].sum().values\n",
    "# for rect, label in zip(rects, labels):\n",
    "#     height = rect.get_height()\n",
    "#     ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments with Multiple Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rowSums = data_raw.iloc[:,2:].sum(axis=1)\n",
    "# multiLabel_counts = rowSums.value_counts()\n",
    "# multiLabel_counts = multiLabel_counts.iloc[1:]\n",
    "# sns.set(font_scale = 2)\n",
    "# plt.figure(figsize=(15,8))\n",
    "# ax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)\n",
    "# plt.title(\"Comments having multiple labels \")\n",
    "# plt.ylabel('Number of comments', fontsize=18)\n",
    "# plt.xlabel('Number of labels', fontsize=18)\n",
    "# #adding the text labels\n",
    "# rects = ax.patches\n",
    "# labels = multiLabel_counts.values\n",
    "# for rect, label in zip(rects, labels):\n",
    "#     height = rect.get_height()\n",
    "#     ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "data = data_raw\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "def clean_total(sentence):\n",
    "    return cleanHtml(sentence), cleanPunc(sentence), keepAlpha(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comment_text'] = data['comment_text'].str.lower()\n",
    "data['comment_text'] = data['comment_text'].apply(cleanHtml)\n",
    "data['comment_text'] = data['comment_text'].apply(cleanPunc)\n",
    "data['comment_text'] = data['comment_text'].apply(keepAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "data['comment_text'] = data['comment_text'].apply(removeStopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, random_state=42, test_size=0.30, shuffle=True)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "vectorizer.fit(train['comment_text'])\n",
    "# vectorizer.fit(test['comment_text'])\n",
    "x_train = vectorizer.transform(train['comment_text'])\n",
    "y_train = train.drop(labels = ['id','comment_text'], axis=1)\n",
    "x_test = vectorizer.transform(test['comment_text'])\n",
    "y_test = test.drop(labels = ['id','comment_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, matthews_corrcoef, f1_score, log_loss\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One vs Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111699"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag', class_weight='automatic'))),\n",
    "            ])\n",
    "\n",
    "def pipeline_model_training(category, dataTrain, targetTrain):\n",
    "    # Training logistic regression model on train data\n",
    "    return LogReg_pipeline.fit(dataTrain, targetTrain)\n",
    "\n",
    "def pipeline_predict(category, dictPred, dictPredProba, dictHasil, dataTest, targetTest):\n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(dataTest)\n",
    "    dictPredProba['{}'.format(category)] = LogReg_pipeline.predict_proba(dataTest)\n",
    "    dictPred[category] = prediction \n",
    "    dictHasil['{}_accuracy'.format(category)] = accuracy_score(targetTest, prediction)\n",
    "    dictHasil['{}_f1score'.format(category)] = f1_score(targetTest, prediction, average = 'macro')\n",
    "    dictHasil['{}_logloss'.format(category)] = log_loss(targetTest, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dHasil_OvR = {}\n",
    "dPrediction_OvR = {}\n",
    "dPredictionProba_OvR = {}\n",
    "\n",
    "for category in categories:\n",
    "    pipeline_model_training(category, x_train, train[category])\n",
    "    pipeline_predict(category, dPrediction_OvR, dPredictionProba_OvR, dHasil_OvR, x_train, train[category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic_accuracy': 0.948298552359466,\n",
       " 'toxic_f1score': 0.8027261959430682,\n",
       " 'toxic_logloss': 1.7857050901161455,\n",
       " 'severe_toxic_accuracy': 0.9909220315311685,\n",
       " 'severe_toxic_f1score': 0.6232705830962695,\n",
       " 'severe_toxic_logloss': 0.31354245995261193,\n",
       " 'obscene_accuracy': 0.9724617051182195,\n",
       " 'obscene_f1score': 0.8186531737208504,\n",
       " 'obscene_logloss': 0.9511394387290054,\n",
       " 'threat_accuracy': 0.9969471526155114,\n",
       " 'threat_f1score': 0.5021510664295756,\n",
       " 'threat_logloss': 0.10544161318064312,\n",
       " 'insult_accuracy': 0.968218157727464,\n",
       " 'insult_f1score': 0.7633842117615259,\n",
       " 'insult_logloss': 1.097707182090157,\n",
       " 'identity_hate_accuracy': 0.9916830052193842,\n",
       " 'identity_hate_f1score': 0.5508465506756818,\n",
       " 'identity_hate_logloss': 0.28725890174884167}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dHasil_OvR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "listItem = []\n",
    "\n",
    "for i in range(len(dPrediction_OvR['toxic'])):\n",
    "    listItem.append([dPrediction_OvR['toxic'][i],\n",
    "                    dPrediction_OvR['severe_toxic'][i],\n",
    "                    dPrediction_OvR['obscene'][i],\n",
    "                    dPrediction_OvR['threat'][i],\n",
    "                    dPrediction_OvR['insult'][i],\n",
    "                    dPrediction_OvR['identity_hate'][i],])\n",
    "\n",
    "dfPredovr = pd.DataFrame(columns=['ToxicPred', 'SToxicPred','ObscenePred','ThreatPred','InsultPred','IdHatePred'],\n",
    "                     data=listItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testLabel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b849ca855ed1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdfResultovr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdfResultovr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestLabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdfPred\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'testLabel' is not defined"
     ]
    }
   ],
   "source": [
    "dfResultovr = pd.DataFrame\n",
    "\n",
    "dfResultovr = pd.concat([testLabel,dfPred], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToxicovr = dfResultovr[(dfResult['toxic'] != 0) | (dfResultovr['severe_toxic'] != 0) | (dfResultovr['obscene'] != 0)\n",
    "         | (dfResultovr['threat'] != 0) | (dfResultovr['insult'] != 0) | (dfResultovr['identity_hate'] != 0)]\n",
    "\n",
    "tebakanToxicTrueovr = dfResultovr[((dfResult['ToxicPred'] == dfResultovr['toxic']) & (dfResultovr['SToxicPred'] == dfResultovr['severe_toxic']) & (dfResultovr['ObscenePred'] == dfResultovr['obscene'])\n",
    "         & (dfResultovr['ThreatPred'] == dfResultovr['threat']) & (dfResultovr['InsultPred'] == dfResultovr['insult']) & (dfResultovr['IdHatePred'] == dfResultovr['identity_hate'])) & \n",
    "             ((dfResultovr['toxic'] != 0) | (dfResultovr['severe_toxic'] != 0) | (dfResultovr['obscene'] != 0)\n",
    "         | (dfResultovr['threat'] != 0) | (dfResultovr['insult'] != 0) | (dfResultovr['identity_hate'] != 0))]\n",
    "\n",
    "tebakanCleanTrueovr = dfResultovr[((dfResultovr['ToxicPred'] == dfResultovr['toxic']) & (dfResultovr['SToxicPred'] == dfResultovr['severe_toxic']) & (dfResultovr['ObscenePred'] == dfResultovr['obscene'])\n",
    "         & (dfResultovr['ThreatPred'] == dfResultovr['threat']) & (dfResultovr['InsultPred'] == dfResultovr['insult']) & (dfResultovr['IdHatePred'] == dfResultovr['identity_hate'])) & \n",
    "             ((dfResultovr['toxic'] == 0) & (dfResultovr['severe_toxic'] == 0) & (dfResultovr['obscene']== 0)\n",
    "         & (dfResultovr['threat'] == 0) & (dfResultovr['insult'] == 0) & (dfResultovr['identity_hate'] == 0))]\n",
    "\n",
    "tebakanTrueovr = dfResultovr[(dfResultovr['ToxicPred'] == dfResultovr['toxic']) & (dfResultovr['SToxicPred'] == dfResultovr['severe_toxic']) & (dfResultovr['ObscenePred'] == dfResultovr['obscene'])\n",
    "         & (dfResultovr['ThreatPred'] == dfResultovr['threat']) & (dfResultovr['InsultPred'] == dfResultovr['insult']) & (dfResultovr['IdHatePred'] == dfResultovr['identity_hate'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfResultovr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Persentasi Tebakan Benar = ', round(len(tebakanTrueovr)/len(dfResultovr)*100, 2),'%')\n",
    "print('Persentasi Tebakan Toxic Benar = ', round(len(tebakanToxicTrueovr)/len(dataToxicovr)*100, 2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tebakanTrueovr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLabel = pd.read_csv('testLabelFix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = pd.read_csv('testDataFix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest['comment_text'] = dfTest['comment_text'].str.lower()\n",
    "dfTest['comment_text'] = dfTest['comment_text'].apply(cleanHtml)\n",
    "dfTest['comment_text'] = dfTest['comment_text'].apply(cleanPunc)\n",
    "dfTest['comment_text'] = dfTest['comment_text'].apply(keepAlpha)\n",
    "dfTest['comment_text'] = dfTest['comment_text'].apply(removeStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTest = vectorizer.transform(dfTest['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dHasil_test = {}\n",
    "dPrediction_test = {}\n",
    "dPredictionProba_test = {}\n",
    "\n",
    "for category in categories:\n",
    "    pipeline_predict(category, dPrediction_test, dPredictionProba_test, dHasil_test, dataTest, testLabel[category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:    \n",
    "    print(dHasil_test['{}_accuracy'.format(category)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listItem = []\n",
    "\n",
    "for i in range(len(dPrediction_test['toxic'])):\n",
    "    listItem.append([dPrediction_test['toxic'][i],\n",
    "                    dPrediction_test['severe_toxic'][i],\n",
    "                    dPrediction_test['obscene'][i],\n",
    "                    dPrediction_test['threat'][i],\n",
    "                    dPrediction_test['insult'][i],\n",
    "                    dPrediction_test['identity_hate'][i],])\n",
    "\n",
    "dfPred = pd.DataFrame(columns=['ToxicPred', 'SToxicPred','ObscenePred','ThreatPred','InsultPred','IdHatePred'],\n",
    "                     data=listItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResult = pd.DataFrame\n",
    "\n",
    "dfResult = pd.concat([testLabel,dfPred], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = len(dfResult[(dfResult['ToxicPred'] == dfResult['toxic']) & (dfResult['SToxicPred'] == dfResult['severe_toxic']) & (dfResult['ObscenePred'] == dfResult['obscene'])\n",
    "         & (dfResult['ThreatPred'] == dfResult['threat']) & (dfResult['InsultPred'] == dfResult['insult']) & (dfResult['IdHatePred'] == dfResult['identity_hate'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Persentasi Tebakan Benar = ', round(a/len(dfResult)*100, 2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfResult[(dfResult['toxic'] == 0) & (dfResult['severe_toxic'] == 0) & (dfResult['obscene']== 0)\n",
    "         & (dfResult['threat'] == 0) & (dfResult['insult'] == 0) & (dfResult['identity_hate'] == 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfResult[(dfResult['toxic'] != 0) | (dfResult['severe_toxic'] != 0) | (dfResult['obscene'] != 0)\n",
    "         | (dfResult['threat'] != 0) | (dfResult['insult'] != 0) | (dfResult['identity_hate'] != 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToxic = dfResult[(dfResult['toxic'] != 0) | (dfResult['severe_toxic'] != 0) | (dfResult['obscene'] != 0)\n",
    "         | (dfResult['threat'] != 0) | (dfResult['insult'] != 0) | (dfResult['identity_hate'] != 0)]\n",
    "\n",
    "tebakanToxicTrue = dfResult[((dfResult['ToxicPred'] == dfResult['toxic']) & (dfResult['SToxicPred'] == dfResult['severe_toxic']) & (dfResult['ObscenePred'] == dfResult['obscene'])\n",
    "         & (dfResult['ThreatPred'] == dfResult['threat']) & (dfResult['InsultPred'] == dfResult['insult']) & (dfResult['IdHatePred'] == dfResult['identity_hate'])) & \n",
    "             ((dfResult['toxic'] != 0) | (dfResult['severe_toxic'] != 0) | (dfResult['obscene'] != 0)\n",
    "         | (dfResult['threat'] != 0) | (dfResult['insult'] != 0) | (dfResult['identity_hate'] != 0))]\n",
    "\n",
    "tebakanCleanTrue = dfResult[((dfResult['ToxicPred'] == dfResult['toxic']) & (dfResult['SToxicPred'] == dfResult['severe_toxic']) & (dfResult['ObscenePred'] == dfResult['obscene'])\n",
    "         & (dfResult['ThreatPred'] == dfResult['threat']) & (dfResult['InsultPred'] == dfResult['insult']) & (dfResult['IdHatePred'] == dfResult['identity_hate'])) & \n",
    "             ((dfResult['toxic'] == 0) & (dfResult['severe_toxic'] == 0) & (dfResult['obscene']== 0)\n",
    "         & (dfResult['threat'] == 0) & (dfResult['insult'] == 0) & (dfResult['identity_hate'] == 0))]\n",
    "\n",
    "tebakanTrue = dfResult[(dfResult['ToxicPred'] == dfResult['toxic']) & (dfResult['SToxicPred'] == dfResult['severe_toxic']) & (dfResult['ObscenePred'] == dfResult['obscene'])\n",
    "         & (dfResult['ThreatPred'] == dfResult['threat']) & (dfResult['InsultPred'] == dfResult['insult']) & (dfResult['IdHatePred'] == dfResult['identity_hate'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tebakanToxicTrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Persentasi Tebakan Benar = ', round(len(tebakanTrue)/len(dfResult)*100, 2),'%')\n",
    "print('Persentasi Tebakan Toxic Benar = ', round(len(tebakanToxicTrue)/len(dataToxic)*100, 2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pake Proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_toxic = []\n",
    "test_severe_toxic = []\n",
    "test_obscene = []\n",
    "test_threat = []\n",
    "test_insult = []\n",
    "test_identity_hate = []\n",
    "lTest = [test_toxic, test_severe_toxic, test_obscene, test_threat, test_insult, test_identity_hate]\n",
    "for category, item in zip(categories,lTest):\n",
    "    for i in range(len(dPredictionProba_test['toxic'])):\n",
    "        if dPredictionProba_test[category][i][1] >= 0.00:\n",
    "            item.append(1)\n",
    "        else:\n",
    "            item.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listItem = []\n",
    "\n",
    "for i in range(len(dPredictionProba_test['toxic'])):\n",
    "    listItem.append([test_toxic[i],\n",
    "                    test_severe_toxic[i],\n",
    "                    test_obscene[i],\n",
    "                    test_threat[i],\n",
    "                    test_insult[i],\n",
    "                    test_identity_hate[i]])\n",
    "\n",
    "dfPredProba = pd.DataFrame(columns=['ToxicPred', 'SToxicPred','ObscenePred','ThreatPred','InsultPred','IdHatePred'],\n",
    "                     data=listItem)\n",
    "\n",
    "dfResultProba = pd.DataFrame\n",
    "dfResultProba = pd.concat([testLabel,dfPredProba], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToxicProba = dfResultProba[(dfResult['toxic'] != 0) | (dfResultProba['severe_toxic'] != 0) | (dfResultProba['obscene'] != 0)\n",
    "             | (dfResultProba['threat'] != 0) | (dfResultProba['insult'] != 0) | (dfResultProba['identity_hate'] != 0)]\n",
    "\n",
    "tebakanToxicTrueProba = dfResultProba[((dfResult['ToxicPred'] == dfResultProba['toxic']) & (dfResult['SToxicPred'] == dfResultProba['severe_toxic']) & (dfResultProba['ObscenePred'] == dfResultProba['obscene'])\n",
    "         & (dfResultProba['ThreatPred'] == dfResultProba['threat']) & (dfResult['InsultPred'] == dfResultProba['insult']) & (dfResult['IdHatePred'] == dfResultProba['identity_hate'])) & \n",
    "             ((dfResultProba['toxic'] != 0) | (dfResultProba['severe_toxic'] != 0) | (dfResultProba['obscene'] != 0)\n",
    "         | (dfResultProba['threat'] != 0) | (dfResultProba['insult'] != 0) | (dfResultProba['identity_hate'] != 0))]\n",
    "\n",
    "tebakanCleanTrueProba = dfResultProba[((dfResult['ToxicPred'] == dfResultProba['toxic']) & (dfResultProba['SToxicPred'] == dfResultProba['severe_toxic']) & (dfResultProba['ObscenePred'] == dfResultProba['obscene'])\n",
    "         & (dfResultProba['ThreatPred'] == dfResultProba['threat']) & (dfResultProba['InsultPred'] == dfResultProba['insult']) & (dfResultProba['IdHatePred'] == dfResultProba['identity_hate'])) & \n",
    "             ((dfResultProba['toxic'] == 0) & (dfResultProba['severe_toxic'] == 0) & (dfResultProba['obscene']== 0)\n",
    "         & (dfResultProba['threat'] == 0) & (dfResultProba['insult'] == 0) & (dfResultProba['identity_hate'] == 0))]\n",
    "\n",
    "tebakanTrueProba = dfResultProba[(dfResultProba['ToxicPred'] == dfResultProba['toxic']) & (dfResultProba['SToxicPred'] == dfResultProba['severe_toxic']) & (dfResultProba['ObscenePred'] == dfResultProba['obscene'])\n",
    "         & (dfResultProba['ThreatPred'] == dfResultProba['threat']) & (dfResultProba['InsultPred'] == dfResultProba['insult']) & (dfResultProba['IdHatePred'] == dfResultProba['identity_hate'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Persentasi Tebakan Benar = ', round(len(tebakanTrueProba)/len(dfResultProba)*100, 2),'%')\n",
    "print('Persentasi Tebakan Toxic Benar = ', round(len(tebakanToxicTrueProba)/len(dataToxicProba)*100, 2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
