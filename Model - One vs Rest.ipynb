{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('trainMaster.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train['comment_text']\n",
    "\n",
    "y1 = train['toxic']\n",
    "y2 = train['severe_toxic']\n",
    "y3 = train['obscene']\n",
    "y4 = train['threat']\n",
    "y5 = train['insult']\n",
    "y6 = train['identity_hate']\n",
    "y7 = train['clean']\n",
    "\n",
    "datay = [y1, y2, y3, y4, y5, y6, y7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dSplit = {}\n",
    "for j in range(1,8):\n",
    "        X_train, X_test, dSplit['y{}_train'.format(j)], dSplit['y{}_test'.format(j)] = train_test_split(x, datay[j-1], test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OwO\n"
     ]
    }
   ],
   "source": [
    "print('OwO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 5\n",
    "# kf = KFold(n_splits=K, shuffle=True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dFold = {}\n",
    "# for i in range(1,8):\n",
    "#     dFold['Hasilfold{}'.format(i)] = kf.split(x,datay[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzer for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = []\n",
    "for words in stopwords.words('english'):\n",
    "    s = [char for char in words if char not in string.punctuation]\n",
    "    stop.append(''.join(s))\n",
    "stop.extend(['may','also','across','among','beside','however','yet','within'])\n",
    "\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "# def stemming(sentence):\n",
    "#     stemSentence = \"\"\n",
    "#     for word in sentence.split():\n",
    "#         stem = stemmer.stem(word)\n",
    "#         stemSentence += stem\n",
    "#         stemSentence += \" \"\n",
    "#     stemSentence = stemSentence.strip()\n",
    "#     return stemSentence\n",
    "\n",
    "def process_normal(text):\n",
    "    return [word for word in text.split() if word.lower() not in stop]\n",
    "\n",
    "def process_nocase(text):\n",
    "    return [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "\n",
    "def process_nopunc(text):\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stop]\n",
    "\n",
    "def process_nocasepunc(text):\n",
    "    nocasepunc = [char for char in text if char not in string.punctuation]\n",
    "    nocasepunc = ''.join(nocasepunc)\n",
    "    return [word.lower() for word in nocasepunc.split() if word.lower() not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listAnalyzer = [process_normal, process_nocase,process_nopunc,process_nocasepunc]\n",
    "# nama = ['normal','noCase','noPunc','noCasePunc']\n",
    "# dPP = {}\n",
    "# dPP2 = {}\n",
    "\n",
    "# # buat analyzer\n",
    "# for i in range(0,4): \n",
    "# #     buat n_gram\n",
    "#     for j in range(1,4):\n",
    "#         dPP['pp_{}_ngram_{}'.format(nama[i],j)] = Pipeline([\n",
    "#             ('bow', CountVectorizer(analyzer=listAnalyzer[i], ngram_range=(1,j), lowercase=False)),\n",
    "#             ('classifier', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "#         ])\n",
    "\n",
    "#         # With TFIDF\n",
    "#         dPP2['pp2_{}_ngram_{}'.format(nama[i],j)] = Pipeline([\n",
    "#             ('bow', CountVectorizer(analyzer=listAnalyzer[i], ngram_range=(1,j), lowercase=False)),\n",
    "#             ('tfidf', TfidfTransformer()),\n",
    "#             ('classifier', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)))),\n",
    "#         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Processing toxic comments...**\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle the task to send it to the workers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\backend\\queues.py\", line 150, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\backend\\reduction.py\", line 243, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\backend\\reduction.py\", line 236, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\cloudpickle\\cloudpickle.py\", line 284, in dump\n    return Pickler.dump(self, obj)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 437, in dump\n    self.save(obj)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 887, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 816, in save_list\n    self._batch_appends(obj)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 843, in _batch_appends\n    save(tmp[0])\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 771, in save_tuple\n    save(element)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 771, in save_tuple\n    save(element)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 786, in save_tuple\n    save(element)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 816, in save_list\n    self._batch_appends(obj)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 843, in _batch_appends\n    save(tmp[0])\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\pickle.py\", line 510, in save\n    rv = reduce(obj)\n  File \"C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_memmapping_reducer.py\", line 361, in __call__\n    return (loads, (dumps(a, protocol=HIGHEST_PROTOCOL),))\nMemoryError\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b7a9810a0bb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Training logistic regression model on train data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mLogReg_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# calculating test accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[1;34m\"not %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m                 self.label_binarizer_.classes_[i]])\n\u001b[1;32m--> 215\u001b[1;33m             for i, column in enumerate(columns))\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
     ]
    }
   ],
   "source": [
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "categories = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "for category in categories:\n",
    "    print('**Processing {} comments...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(X_train, train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OwO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppKey = list(dPP.keys())\n",
    "pp2Key = list(dPP2.keys())\n",
    "\n",
    "dPredict_train = {}\n",
    "dPredict_test = {}\n",
    "dPredictProba_train = {}\n",
    "dPredictProba_test = {}\n",
    "\n",
    "dPredict2_train = {}\n",
    "dPredict2_test = {}\n",
    "dPredict2Proba_train = {}\n",
    "dPredict2Proba_test = {}\n",
    "\n",
    "for item, item2 in zip(ppKey, pp2Key):\n",
    "    for i in range(1,8):\n",
    "#         Pipeline\n",
    "        dPP[item].fit(X_train, dSplit['y{}_train'.format(i)])\n",
    "        \n",
    "        dPredict_train['predictions_train_y{}_{}'.format(i,item)] = dPP[item].predict(X_train)\n",
    "        dPredictProba_train['predictions_train_y{}_{}'.format(i,item)] = dPP[item].predict_proba(X_train)\n",
    "        \n",
    "        dPredict_test['predictions_test_y{}_{}'.format(i,item)] = dPP[item].predict(X_test)\n",
    "        dPredictProba_test['predictions_test_y{}_{}'.format(i,item)] = dPP[item].predict_proba(X_test)\n",
    "        \n",
    "#         Pipeline2\n",
    "        dPP2[item2].fit(X_train, dSplit['y{}_train'.format(i)])\n",
    "    \n",
    "        dPredict2_train['predictions2_train_y{}_{}'.format(i,item2)] = dPP2[item2].predict(X_train)\n",
    "        dPredict2Proba_train['predictions2_train_y{}_{}'.format(i,item2)] = dPP2[item2].predict_proba(X_train)\n",
    "        \n",
    "        dPredict2_test['predictions2_test_y{}_{}'.format(i,item2)] = dPP2[item2].predict(X_test)\n",
    "        dPredict2Proba_test['predictions2_test_y{}_{}'.format(i,item2)] = dPP2[item2].predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OwO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, matthews_corrcoef, f1_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_error(X_train, y_train, predictTrain, predictProbaTrain):\n",
    "    matt = matthews_corrcoef(y_train, predictTrain)\n",
    "    f1 = f1_score(y_train, predictTrain, average = 'macro')\n",
    "    report = classification_report(y_train, predictTrain)\n",
    "    roc_auc = roc_auc_score(y_train, predictProbaTrain[:,1])\n",
    "    accuracy = accuracy_score(y_train, predictTrain)\n",
    "    confMatrix = confusion_matrix(y_train, predictTrain)\n",
    "    logloss = log_loss(y_train, predictProbaTrain)\n",
    "    return {\n",
    "        'report':report,\n",
    "        'matthew':matt,\n",
    "        'f1':f1,\n",
    "        'roc':roc_auc,\n",
    "        'accuracy':accuracy,\n",
    "        'confusion':confMatrix,\n",
    "        'logloss':logloss\n",
    "    }\n",
    "\n",
    "def calc_validation_error(X_test, y_test, predictTest, predictProbaTest):\n",
    "    matt = matthews_corrcoef(y_test, predictTest)\n",
    "    f1 = f1_score(y_test, predictTest, average = 'macro')\n",
    "    report = classification_report(y_test, predictTest)\n",
    "    roc_auc = roc_auc_score(y_test, predictProbaTest[:,1])\n",
    "    accuracy = accuracy_score(y_test, predictTest)\n",
    "    confMatrix = confusion_matrix(y_test, predictTest)\n",
    "    logloss = log_loss(y_test, predictProbaTest)\n",
    "    return {\n",
    "        'report':report,\n",
    "        'matthew':matt,\n",
    "        'f1':f1,\n",
    "        'roc':roc_auc,\n",
    "        'accuracy':accuracy,\n",
    "        'confusion':confMatrix,\n",
    "        'logloss':logloss\n",
    "    }\n",
    "\n",
    "def calc_metrics(X_train, y_train, X_test, y_test, predictTrain, predictProbaTrain, predictTest, predictProbaTest):\n",
    "    train_error = calc_train_error(X_train, y_train, predictTrain, predictProbaTrain)\n",
    "    validation_error = calc_validation_error(X_test, y_test, predictTest, predictProbaTest)\n",
    "    return train_error, validation_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dToxic_train = {}\n",
    "dToxic_test = {}\n",
    "dToxic2_train = {}\n",
    "dToxic2_test = {}\n",
    "\n",
    "for item in ppKey:\n",
    "    dToxic_train['hasil_train_{}'.format(item)], dToxic_test['hasil_test_{}'.format(item)]= calc_metrics(X_train, dSplit['y1_train'], X_test, dSplit['y1_test'], \n",
    "                                                                                                          dPredict_train['predictions_train_y1_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_train['predictions_train_y1_{}'.format(item)],\n",
    "                                                                                                          dPredict_test['predictions_test_y1_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_test['predictions_test_y1_{}'.format(item)])\n",
    "\n",
    "for item2 in pp2Key:\n",
    "    dToxic2_train['hasil_train_{}'.format(item2)], dToxic2_test['hasil_test_{}'.format(item2)]= calc_metrics(X_train, dSplit['y1_train'], X_test, dSplit['y1_test'], \n",
    "                                                                                                          dPredict2_train['predictions2_train_y1_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_train['predictions2_train_y1_{}'.format(item2)],\n",
    "                                                                                                          dPredict2_test['predictions2_test_y1_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_test['predictions2_test_y1_{}'.format(item2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dToxic_train_key = list(dToxic_train.keys())\n",
    "dToxic_test_key = list(dToxic_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dToxic_train_key, dToxic_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dToxic_train[train]['accuracy'],\n",
    "                     dToxic_test[test]['accuracy'],\n",
    "                     dToxic_train[train]['roc'],\n",
    "                     dToxic_test[test]['roc'],\n",
    "                     dToxic_train[train]['f1'],\n",
    "                     dToxic_test[test]['f1'],\n",
    "                     dToxic_train[train]['matthew'],\n",
    "                     dToxic_test[test]['matthew'],\n",
    "                     dToxic_train[train]['logloss'],\n",
    "                     dToxic_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfToxicResult = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfToxicResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dToxic2_train_key = list(dToxic2_train.keys())\n",
    "dToxic2_test_key = list(dToxic2_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dToxic2_train_key, dToxic2_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dToxic2_train[train]['accuracy'],\n",
    "                     dToxic2_test[test]['accuracy'],\n",
    "                     dToxic2_train[train]['roc'],\n",
    "                     dToxic2_test[test]['roc'],\n",
    "                     dToxic2_train[train]['f1'],\n",
    "                     dToxic2_test[test]['f1'],\n",
    "                     dToxic2_train[train]['matthew'],\n",
    "                     dToxic2_test[test]['matthew'],\n",
    "                     dToxic2_train[train]['logloss'],\n",
    "                     dToxic2_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfToxic2Result = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfToxic2Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(confusion_matrix(dSplit['y1_test'], predictions))\n",
    "# print('\\n')\n",
    "# print(classification_report(dSplit['y1_test'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severe Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dSToxic_train = {}\n",
    "dSToxic_test = {}\n",
    "dSToxic2_train = {}\n",
    "dSToxic2_test = {}\n",
    "\n",
    "for item in ppKey:\n",
    "    dSToxic_train['hasil_train_{}'.format(item)], dSToxic_test['hasil_test_{}'.format(item)]= calc_metrics(X_train, dSplit['y2_train'], X_test, dSplit['y2_test'], \n",
    "                                                                                                          dPredict_train['predictions_train_y2_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_train['predictions_train_y2_{}'.format(item)],\n",
    "                                                                                                          dPredict_test['predictions_test_y2_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_test['predictions_test_y2_{}'.format(item)])\n",
    "\n",
    "for item2 in pp2Key:\n",
    "    dSToxic2_train['hasil_train_{}'.format(item2)], dSToxic2_test['hasil_test_{}'.format(item2)]= calc_metrics(X_train, dSplit['y2_train'], X_test, dSplit['y2_test'], \n",
    "                                                                                                          dPredict2_train['predictions2_train_y2_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_train['predictions2_train_y2_{}'.format(item2)],\n",
    "                                                                                                          dPredict2_test['predictions2_test_y2_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_test['predictions2_test_y2_{}'.format(item2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dSToxic_train_key = list(dSToxic_train.keys())\n",
    "dSToxic_test_key = list(dSToxic_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dSToxic_train_key, dSToxic_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dSToxic_train[train]['accuracy'],\n",
    "                     dSToxic_test[test]['accuracy'],\n",
    "                     dSToxic_train[train]['roc'],\n",
    "                     dSToxic_test[test]['roc'],\n",
    "                     dSToxic_train[train]['f1'],\n",
    "                     dSToxic_test[test]['f1'],\n",
    "                     dSToxic_train[train]['matthew'],\n",
    "                     dSToxic_test[test]['matthew'],\n",
    "                     dSToxic_train[train]['logloss'],\n",
    "                     dSToxic_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfSToxicResult = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfSToxicResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dSToxic2_train_key = list(dSToxic2_train.keys())\n",
    "dSToxic2_test_key = list(dSToxic2_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dSToxic2_train_key, dSToxic2_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dSToxic2_train[train]['accuracy'],\n",
    "                     dSToxic2_test[test]['accuracy'],\n",
    "                     dSToxic2_train[train]['roc'],\n",
    "                     dSToxic2_test[test]['roc'],\n",
    "                     dSToxic2_train[train]['f1'],\n",
    "                     dSToxic2_test[test]['f1'],\n",
    "                     dSToxic2_train[train]['matthew'],\n",
    "                     dSToxic2_test[test]['matthew'],\n",
    "                     dSToxic2_train[train]['logloss'],\n",
    "                     dSToxic2_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfSToxic2Result = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfSToxic2Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obscene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dObscene_train = {}\n",
    "dObscene_test = {}\n",
    "dObscene2_train = {}\n",
    "dObscene2_test = {}\n",
    "\n",
    "for item in ppKey:\n",
    "    dObscene_train['hasil_train_{}'.format(item)], dObscene_test['hasil_test_{}'.format(item)]= calc_metrics(X_train, dSplit['y3_train'], X_test, dSplit['y3_test'], \n",
    "                                                                                                          dPredict_train['predictions_train_y3_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_train['predictions_train_y3_{}'.format(item)],\n",
    "                                                                                                          dPredict_test['predictions_test_y3_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_test['predictions_test_y3_{}'.format(item)])\n",
    "\n",
    "for item2 in pp2Key:\n",
    "    dObscene2_train['hasil_train_{}'.format(item2)], dObscene2_test['hasil_test_{}'.format(item2)]= calc_metrics(X_train, dSplit['y3_train'], X_test, dSplit['y3_test'], \n",
    "                                                                                                          dPredict2_train['predictions2_train_y3_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_train['predictions2_train_y3_{}'.format(item2)],\n",
    "                                                                                                          dPredict2_test['predictions2_test_y3_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_test['predictions2_test_y3_{}'.format(item2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dObscene_train_key = list(dObscene_train.keys())\n",
    "dObscene_test_key = list(dObscene_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dObscene_train_key, dObscene_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dObscene_train[train]['accuracy'],\n",
    "                     dObscene_test[test]['accuracy'],\n",
    "                     dObscene_train[train]['roc'],\n",
    "                     dObscene_test[test]['roc'],\n",
    "                     dObscene_train[train]['f1'],\n",
    "                     dObscene_test[test]['f1'],\n",
    "                     dObscene_train[train]['matthew'],\n",
    "                     dObscene_test[test]['matthew'],\n",
    "                     dObscene_train[train]['logloss'],\n",
    "                     dObscene_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfObsceneResult = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfObsceneResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dObscene2_train_key = list(dObscene2_train.keys())\n",
    "dObscene2_test_key = list(dObscene2_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dObscene2_train_key, dObscene2_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dObscene2_train[train]['accuracy'],\n",
    "                     dObscene2_test[test]['accuracy'],\n",
    "                     dObscene2_train[train]['roc'],\n",
    "                     dObscene2_test[test]['roc'],\n",
    "                     dObscene2_train[train]['f1'],\n",
    "                     dObscene2_test[test]['f1'],\n",
    "                     dObscene2_train[train]['matthew'],\n",
    "                     dObscene2_test[test]['matthew'],\n",
    "                     dObscene2_train[train]['logloss'],\n",
    "                     dObscene2_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfObscene2Result = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfObscene2Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dThreat_train = {}\n",
    "dThreat_test = {}\n",
    "dThreat2_train = {}\n",
    "dThreat2_test = {}\n",
    "\n",
    "for item in ppKey:\n",
    "    dThreat_train['hasil_train_{}'.format(item)], dThreat_test['hasil_test_{}'.format(item)]= calc_metrics(X_train, dSplit['y4_train'], X_test, dSplit['y4_test'], \n",
    "                                                                                                          dPredict_train['predictions_train_y4_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_train['predictions_train_y4_{}'.format(item)],\n",
    "                                                                                                          dPredict_test['predictions_test_y4_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_test['predictions_test_y4_{}'.format(item)])\n",
    "\n",
    "for item2 in pp2Key:\n",
    "    dThreat2_train['hasil_train_{}'.format(item2)], dThreat2_test['hasil_test_{}'.format(item2)]= calc_metrics(X_train, dSplit['y4_train'], X_test, dSplit['y4_test'], \n",
    "                                                                                                          dPredict2_train['predictions2_train_y4_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_train['predictions2_train_y4_{}'.format(item2)],\n",
    "                                                                                                          dPredict2_test['predictions2_test_y4_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_test['predictions2_test_y4_{}'.format(item2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dThreat_train_key = list(dThreat_train.keys())\n",
    "dThreat_test_key = list(dThreat_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dThreat_train_key, dThreat_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dObscene_train[train]['accuracy'],\n",
    "                     dObscene_test[test]['accuracy'],\n",
    "                     dObscene_train[train]['roc'],\n",
    "                     dObscene_test[test]['roc'],\n",
    "                     dObscene_train[train]['f1'],\n",
    "                     dObscene_test[test]['f1'],\n",
    "                     dObscene_train[train]['matthew'],\n",
    "                     dObscene_test[test]['matthew'],\n",
    "                     dObscene_train[train]['logloss'],\n",
    "                     dObscene_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfThreatResult = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfThreatResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dThreat2_train_key = list(dThreat2_train.keys())\n",
    "dThreat2_test_key = list(dThreat2_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dThreat2_train_key, dThreat2_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dObscene2_train[train]['accuracy'],\n",
    "                     dObscene2_test[test]['accuracy'],\n",
    "                     dObscene2_train[train]['roc'],\n",
    "                     dObscene2_test[test]['roc'],\n",
    "                     dObscene2_train[train]['f1'],\n",
    "                     dObscene2_test[test]['f1'],\n",
    "                     dObscene2_train[train]['matthew'],\n",
    "                     dObscene2_test[test]['matthew'],\n",
    "                     dObscene2_train[train]['logloss'],\n",
    "                     dObscene2_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfThreat2Result = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfThreat2Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dInsult_train = {}\n",
    "dInsult_test = {}\n",
    "dInsult2_train = {}\n",
    "dInsult2_test = {}\n",
    "\n",
    "for item in ppKey:\n",
    "    dInsult_train['hasil_train_{}'.format(item)], dInsult_test['hasil_test_{}'.format(item)]= calc_metrics(X_train, dSplit['y5_train'], X_test, dSplit['y5_test'], \n",
    "                                                                                                          dPredict_train['predictions_train_y5_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_train['predictions_train_y5_{}'.format(item)],\n",
    "                                                                                                          dPredict_test['predictions_test_y5_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_test['predictions_test_y5_{}'.format(item)])\n",
    "\n",
    "for item2 in pp2Key:\n",
    "    dInsult2_train['hasil_train_{}'.format(item2)], dInsult2_test['hasil_test_{}'.format(item2)]= calc_metrics(X_train, dSplit['y5_train'], X_test, dSplit['y5_test'], \n",
    "                                                                                                          dPredict2_train['predictions2_train_y5_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_train['predictions2_train_y5_{}'.format(item2)],\n",
    "                                                                                                          dPredict2_test['predictions2_test_y5_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_test['predictions2_test_y5_{}'.format(item2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dInsult_train_key = list(dInsult_train.keys())\n",
    "dInsult_test_key = list(dInsult_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dInsult_train_key, dInsult_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dInsult_train[train]['accuracy'],\n",
    "                     dInsult_test[test]['accuracy'],\n",
    "                     dInsult_train[train]['roc'],\n",
    "                     dInsult_test[test]['roc'],\n",
    "                     dInsult_train[train]['f1'],\n",
    "                     dInsult_test[test]['f1'],\n",
    "                     dInsult_train[train]['matthew'],\n",
    "                     dInsult_test[test]['matthew'],\n",
    "                     dInsult_train[train]['logloss'],\n",
    "                     dInsult_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfInsultResult = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfInsultResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dInsult2_train_key = list(dInsult2_train.keys())\n",
    "dInsult2_test_key = list(dInsult2_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dInsult2_train_key, dInsult2_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dInsult2_train[train]['accuracy'],\n",
    "                     dInsult2_test[test]['accuracy'],\n",
    "                     dInsult2_train[train]['roc'],\n",
    "                     dInsult2_test[test]['roc'],\n",
    "                     dInsult2_train[train]['f1'],\n",
    "                     dInsult2_test[test]['f1'],\n",
    "                     dInsult2_train[train]['matthew'],\n",
    "                     dInsult2_test[test]['matthew'],\n",
    "                     dInsult2_train[train]['logloss'],\n",
    "                     dInsult2_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfInsult2Result = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfInsult2Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dIdHate_train = {}\n",
    "dIdHate_test = {}\n",
    "dIdHate2_train = {}\n",
    "dIdHate2_test = {}\n",
    "\n",
    "for item in ppKey:\n",
    "    dIdHate_train['hasil_train_{}'.format(item)], dIdHate_test['hasil_test_{}'.format(item)]= calc_metrics(X_train, dSplit['y6_train'], X_test, dSplit['y6_test'], \n",
    "                                                                                                          dPredict_train['predictions_train_y6_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_train['predictions_train_y6_{}'.format(item)],\n",
    "                                                                                                          dPredict_test['predictions_test_y6_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_test['predictions_test_y6_{}'.format(item)])\n",
    "\n",
    "for item2 in pp2Key:\n",
    "    dIdHate2_train['hasil_train_{}'.format(item2)], dIdHate2_test['hasil_test_{}'.format(item2)]= calc_metrics(X_train, dSplit['y6_train'], X_test, dSplit['y6_test'], \n",
    "                                                                                                          dPredict2_train['predictions2_train_y6_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_train['predictions2_train_y6_{}'.format(item2)],\n",
    "                                                                                                          dPredict2_test['predictions2_test_y6_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_test['predictions2_test_y6_{}'.format(item2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dIdHate_train_key = list(dIdHate_train.keys())\n",
    "dIdHate_test_key = list(dIdHate_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dIdHate_train_key, dIdHate_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dIdHate_train[train]['accuracy'],\n",
    "                     dIdHate_test[test]['accuracy'],\n",
    "                     dIdHate_train[train]['roc'],\n",
    "                     dIdHate_test[test]['roc'],\n",
    "                     dIdHate_train[train]['f1'],\n",
    "                     dIdHate_test[test]['f1'],\n",
    "                     dIdHate_train[train]['matthew'],\n",
    "                     dIdHate_test[test]['matthew'],\n",
    "                     dIdHate_train[train]['logloss'],\n",
    "                     dIdHate_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfIdHateResult = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfIdHateResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dIdHate2_train_key = list(dIdHate2_train.keys())\n",
    "dIdHate2_test_key = list(dIdHate2_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dIdHate2_train_key, dIdHate2_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dIdHate2_train[train]['accuracy'],\n",
    "                     dIdHate2_test[test]['accuracy'],\n",
    "                     dIdHate2_train[train]['roc'],\n",
    "                     dIdHate2_test[test]['roc'],\n",
    "                     dIdHate2_train[train]['f1'],\n",
    "                     dIdHate2_test[test]['f1'],\n",
    "                     dIdHate2_train[train]['matthew'],\n",
    "                     dIdHate2_test[test]['matthew'],\n",
    "                     dIdHate2_train[train]['logloss'],\n",
    "                     dIdHate2_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfIdHate2Result = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfIdHate2Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dClean_train = {}\n",
    "dClean_test = {}\n",
    "dClean2_train = {}\n",
    "dClean2_test = {}\n",
    "\n",
    "for item in ppKey:\n",
    "    dClean_train['hasil_train_{}'.format(item)], dClean_test['hasil_test_{}'.format(item)]= calc_metrics(X_train, dSplit['y7_train'], X_test, dSplit['y7_test'], \n",
    "                                                                                                          dPredict_train['predictions_train_y7_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_train['predictions_train_y7_{}'.format(item)],\n",
    "                                                                                                          dPredict_test['predictions_test_y7_{}'.format(item)],\n",
    "                                                                                                          dPredictProba_test['predictions_test_y7_{}'.format(item)])\n",
    "\n",
    "for item2 in pp2Key:\n",
    "    dClean2_train['hasil_train_{}'.format(item2)], dClean2_test['hasil_test_{}'.format(item2)]= calc_metrics(X_train, dSplit['y7_train'], X_test, dSplit['y7_test'], \n",
    "                                                                                                          dPredict2_train['predictions2_train_y7_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_train['predictions2_train_y7_{}'.format(item2)],\n",
    "                                                                                                          dPredict2_test['predictions2_test_y7_{}'.format(item2)],\n",
    "                                                                                                          dPredict2Proba_test['predictions2_test_y7_{}'.format(item2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dClean_train_key = list(dClean_train.keys())\n",
    "dClean_test_key = list(dClean_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dClean_train_key, dClean_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dClean_train[train]['accuracy'],\n",
    "                     dClean_test[test]['accuracy'],\n",
    "                     dClean_train[train]['roc'],\n",
    "                     dClean_test[test]['roc'],\n",
    "                     dClean_train[train]['f1'],\n",
    "                     dClean_test[test]['f1'],\n",
    "                     dClean_train[train]['matthew'],\n",
    "                     dClean_test[test]['matthew'],\n",
    "                     dClean_train[train]['logloss'],\n",
    "                     dClean_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfCleanResult = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfCleanResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dClean2_train_key = list(dClean2_train.keys())\n",
    "dClean2_test_key = list(dClean2_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dClean2_train_key, dClean2_test_key):\n",
    "    listItem.append(['log_{}'.format(test),\n",
    "                     dClean2_train[train]['accuracy'],\n",
    "                     dClean2_test[test]['accuracy'],\n",
    "                     dClean2_train[train]['roc'],\n",
    "                     dClean2_test[test]['roc'],\n",
    "                     dClean2_train[train]['f1'],\n",
    "                     dClean2_test[test]['f1'],\n",
    "                     dClean2_train[train]['matthew'],\n",
    "                     dClean2_test[test]['matthew'],\n",
    "                     dClean2_train[train]['logloss'],\n",
    "                     dClean2_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfClean2Result = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfClean2Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfToxicResult.to_csv('logisticRegression_toxic.csv',index=False)\n",
    "dfToxic2Result.to_csv('logisticRegression_toxic2.csv',index=False)\n",
    "\n",
    "dfSToxicResult.to_csv('logisticRegression_stoxic.csv',index=False)\n",
    "dfSToxic2Result.to_csv('logisticRegression_stoxic2.csv',index=False)\n",
    "\n",
    "dfObsceneResult.to_csv('logisticRegression_obscene.csv',index=False)\n",
    "dfObscene2Result.to_csv('logisticRegression_obscene2.csv',index=False)\n",
    "\n",
    "dfThreatResult.to_csv('logisticRegression_threat.csv',index=False)\n",
    "dfThreat2Result.to_csv('logisticRegression_threat2.csv',index=False)\n",
    "\n",
    "dfInsultResult.to_csv('logisticRegression_insult.csv',index=False)\n",
    "dfInsult2Result.to_csv('logisticRegression_insult2.csv',index=False)\n",
    "\n",
    "dfIdHateResult.to_csv('logisticRegression_idHate.csv',index=False)\n",
    "dfIdHate2Result.to_csv('logisticRegression_idHate2.csv',index=False)\n",
    "\n",
    "dfCleanResult.to_csv('logisticRegression_clean.csv',index=False)\n",
    "dfClean2Result.to_csv('logisticRegression_clean2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '_log'\n",
    "\n",
    "pickle.dump(dToxic_train, open('dToxic_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dToxic_test, open('dToxic_test{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dToxic2_train, open('dToxic2_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dToxic2_test, open('dToxic2_test{}.sav'.format(model), 'wb'))\n",
    "\n",
    "pickle.dump(dSToxic_train, open('dSToxic_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dSToxic_test, open('dSToxic_test{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dSToxic2_train, open('dSToxic2_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dSToxic2_test, open('dSToxic2_test{}.sav'.format(model), 'wb'))\n",
    "\n",
    "pickle.dump(dObscene_train, open('dObscene_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dObscene_test, open('dObscene_test{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dObscene2_train, open('dObscene2_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dObscene2_test, open('dObscene2_test{}.sav'.format(model), 'wb'))\n",
    "\n",
    "pickle.dump(dThreat_train, open('dThreat_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dThreat_test, open('dThreat_test{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dThreat2_train, open('dThreat2_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dThreat2_test, open('dThreat2_test{}.sav'.format(model), 'wb'))\n",
    "\n",
    "pickle.dump(dInsult_train, open('dInsult_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dInsult_test, open('dInsult_test{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dInsult2_train, open('dInsult2_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dInsult2_test, open('dInsult2_test{}.sav'.format(model), 'wb'))\n",
    "\n",
    "pickle.dump(dIdHate_train, open('dIdHate_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dIdHate_test, open('dIdHate_test{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dIdHate2_train, open('dIdHate2_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dIdHate2_test, open('dIdHate2_test{}.sav'.format(model), 'wb'))\n",
    "\n",
    "pickle.dump(dClean_train, open('dClean_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dClean_test, open('dClean_test{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dClean2_train, open('dClean2_train{}.sav'.format(model), 'wb'))\n",
    "pickle.dump(dClean2_test, open('dClean2_test{}.sav'.format(model), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dPredict_train, open('dPredict_train.sav', 'wb'))\n",
    "pickle.dump(dPredict_test, open('dPredict_test.sav', 'wb'))\n",
    "pickle.dump(dPredictProba_train, open('dPredictProba_train.sav', 'wb'))\n",
    "pickle.dump(dPredictProba_test, open('dPredictProba_test.sav', 'wb'))\n",
    "\n",
    "pickle.dump(dPredict2_train, open('dPredict2_train.sav', 'wb'))\n",
    "pickle.dump(dPredict2_test, open('dPredict2_test.sav', 'wb'))\n",
    "pickle.dump(dPredict2Proba_train, open('dPredict2Proba_train.sav', 'wb'))\n",
    "pickle.dump(dPredict2Proba_test, open('dPredict2Proba_test.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
