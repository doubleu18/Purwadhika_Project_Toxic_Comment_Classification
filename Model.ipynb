{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (pd.read_csv)\n",
    "trainNoStop = pd.read_csv('trainNoStop.csv')\n",
    "trainNoPunc = pd.read_csv('trainNoPunc.csv')\n",
    "trainNoCasePunc = pd.read_csv('trainNoCasePunc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = trainNoStop[['toxic','severe_toxic','obscene','threat','insult','identity_hate','toxic_level','clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = trainNoStop['comment_text']\n",
    "x2 = trainNoPunc['comment_text']\n",
    "x3 = trainNoCasePunc['comment_text']\n",
    "\n",
    "\n",
    "y1 = target['toxic']\n",
    "y2 = target['severe_toxic']\n",
    "y3 = target['obscene']\n",
    "y4 = target['threat']\n",
    "y5 = target['insult']\n",
    "y6 = target['identity_hate']\n",
    "y7 = target['toxic_level']\n",
    "y8 = target['clean']\n",
    "\n",
    "datax = [x1, x2, x3]\n",
    "datay = [y1, y2, y3, y4, y5, y6, y7, y8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i in range(1,4):\n",
    "    for j in range(1,9):\n",
    "        d['X{}_train'.format(i)], d['X{}_test'.format(i)], d['y{}_train'.format(j)], d['y{}_test'.format(j)] = train_test_split(datax[i-1], datay[j-1], test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = []\n",
    "for words in stopwords.words('english'):\n",
    "    s = [char for char in words if char not in string.punctuation]\n",
    "    stop.append(''.join(s))\n",
    "    \n",
    "def process_nostop(text):\n",
    "    return [word for word in text.split() if word.lower() not in stop]\n",
    "\n",
    "def process_nopunc(text):\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stop]\n",
    "\n",
    "def process_nocasepunc(text):\n",
    "    nocasepunc = [char for char in text if char not in string.punctuation]\n",
    "    nocasepunc = ''.join(nocasepunc)\n",
    "    return [word.lower() for word in nocasepunc.split() if word.lower() not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Pipeline([\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# With TFIDF\n",
    "pp2 = Pipeline([\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dPipeline = {}\n",
    "for i in range(1,4):\n",
    "    for j in range(1,7):\n",
    "        dPipeline['pp_x{}_y{}'.format(i,j)] = pp\n",
    "\n",
    "dPipeline2 = {}\n",
    "for i in range(1,4):\n",
    "    for j in range(1,7):\n",
    "        dPipeline2['pp2_x{}_y{}'.format(i,j)] = pp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Series' objects are mutable, thus they cannot be hashed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3584203f8462>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mdPipeline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pp_x{}_y{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X{}_train'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y{}_train'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1490\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m         raise TypeError('{0!r} objects are mutable, thus they cannot be'\n\u001b[1;32m-> 1492\u001b[1;33m                         ' hashed'.format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m   1493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1494\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Series' objects are mutable, thus they cannot be hashed"
     ]
    }
   ],
   "source": [
    "for i in range(1,4):\n",
    "    for j in range(1,7):\n",
    "        dPipeline['pp_x{}_y{}'.format(i,j)].fit(d['X{}_train'.format(i), d['y{}_train'.format(j)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dPipeline['pp_x{}_y{}'.format(1,1)].fit(d['X{}_train'.format(1), d['y{}_train'.format(1)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['Toxic','SToxic','Obscene','Threat','Insult','IdentityHate','Clean']\n",
    "\n",
    "namaToxic = []\n",
    "namaSToxic = []\n",
    "namaObscene = []\n",
    "namaThreat = []\n",
    "namaInsult = []\n",
    "namaIdentityHate = []\n",
    "namaClean = []\n",
    "\n",
    "\n",
    "for item in label:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dToxic_Train',\n",
       " 'dSToxic_Train',\n",
       " 'dObscene_Train',\n",
       " 'dThreat_Train',\n",
       " 'dInsult_Train',\n",
       " 'dIdentityHate_Train',\n",
       " 'dClean_Train']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listNamaTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for nama in nama\n",
    "    dSToxic_train = {}\n",
    "    dSToxic_test = {}\n",
    "    dSToxic2_train = {}\n",
    "    dSToxic2_test = {}\n",
    "\n",
    "    for item in ppKey:\n",
    "        dSToxic_train['hasil_train_{}'.format(item)], dSToxic_test['hasil_test_{}'.format(item)]= calc_metrics(X_train, dSplit['y2_train'], X_test, dSplit['y2_test'], \n",
    "                                                                                                              dPredict_train['predictions_train_y2_{}'.format(item)],\n",
    "                                                                                                              dPredictProba_train['predictions_train_y2_{}'.format(item)],\n",
    "                                                                                                              dPredict_test['predictions_test_y2_{}'.format(item),\n",
    "                                                                                                              dPredictProba_test['predictions_test_y2_{}'.format(item)]])\n",
    "\n",
    "    for item2 in pp2Key:\n",
    "        dSToxic2_Train['hasil_train_{}'.format(item2)], dSToxic2_Test['hasil_test_{}'.format(item2)]= calc_metrics(X_train, dSplit['y2_train'], X_test, dSplit['y2_test'], \n",
    "                                                                                                              dPredict2_train['predictions2_train_y2_{}'.format(item2)],\n",
    "                                                                                                              dPredict2Proba_train['predictions2_train_y2_{}'.format(item2)],\n",
    "                                                                                                              dPredict2_test['predictions2_test_y2_{}'.format(item2),\n",
    "                                                                                                              dPredict2Proba_test['predictions2_test_y2_{}'.format(item2)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dToxic_train_key = list(dToxic_train.keys())\n",
    "dToxic_test_key = list(dToxic_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dToxic_train_key, dToxic_test_key):\n",
    "    listItem.append(['nb_{}'.format(test1),\n",
    "                     dToxic_train[train]['accuracy'],\n",
    "                     dToxic_test[test]['accuracy'],\n",
    "                     dToxic_train[train]['roc'],\n",
    "                     dToxic_test[test]['roc'],\n",
    "                     dToxic_train[train]['f1'],\n",
    "                     dToxic_test[test]['f1'],\n",
    "                     dToxic_train[train]['matthew'],\n",
    "                     dToxic_test[test]['matthew'],\n",
    "                     dToxic_train[train]['logloss'],\n",
    "                     dToxic_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfToxicResult = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfToxicResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dToxic2_train_key = list(c.keys())\n",
    "dToxic2_test_key = list(dToxic2_test.keys())\n",
    "listItem = []\n",
    "\n",
    "for train, test in zip(dToxic2_train_key, dToxic2_test_key):\n",
    "    listItem.append(['nb_{}'.format(test1),\n",
    "                     dToxic2_train[train]['accuracy'],\n",
    "                     dToxic2_test[test]['accuracy'],\n",
    "                     dToxic2_train[train]['roc'],\n",
    "                     dToxic2_test[test]['roc'],\n",
    "                     dToxic2_train[train]['f1'],\n",
    "                     dToxic2_test[test]['f1'],\n",
    "                     dToxic2_train[train]['matthew'],\n",
    "                     dToxic2_test[test]['matthew'],\n",
    "                     dToxic2_train[train]['logloss'],\n",
    "                     dToxic2_test[test]['logloss'],])\n",
    "\n",
    "\n",
    "dfToxic2Result = pd.DataFrame(columns=['Model','Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC', 'Train F1 Score','Test F1 Score', \n",
    "                                      'Train Matthews Corr Coef', 'Test Matthew Corr Coef','Train Log Loss','Test Log Loss'], data=listItem)\n",
    "dfToxicResult2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dThreat_train = {}\n",
    "dThreat_test = {}\n",
    "dThreat2_train = {}\n",
    "dThreat2_test = {}\n",
    "\n",
    "dInsult_train = {}\n",
    "dInsult_test = {}\n",
    "dInsult2_train = {}\n",
    "dInsult2_test = {}\n",
    "\n",
    "dIdHate_train = {}\n",
    "dIdHate_test = {}\n",
    "dIdHate2_train = {}\n",
    "dIdHate2_test = {}\n",
    "\n",
    "dClean_train = {}\n",
    "dClean_test = {}\n",
    "dClean2_train = {}\n",
    "dClean2_test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = ['Toxic',]\n",
    "dClean_train = {}\n",
    "dClean_test = {}\n",
    "dClean2_train = {}\n",
    "dClean2_test = {}\n",
    "\n",
    "\n",
    "pickle.dump(dClean_train, open('dClean_train.sav', 'wb'))\n",
    "pickle.dump(dClean_test, open('dClean_test.sav', 'wb'))\n",
    "pickle.dump(dClean2_train, open('dClean2_train.sav', 'wb'))\n",
    "pickle.dump(dClean2_test, open('dClean2_test.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ayamkucing\n"
     ]
    }
   ],
   "source": [
    "a = 'kucing'\n",
    "\n",
    "print('ayam{}'.format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\Dict GBC\\\\dToxic_train_gbc.sav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-8f975f35a5a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..\\\\Dict GBC\\\\dToxic_train_gbc.sav'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\Dict GBC\\\\dToxic_train_gbc.sav'"
     ]
    }
   ],
   "source": [
    "a = pickle.load(open('..\\\\Dict GBC\\\\dToxic_train_gbc.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['Toxic', 'SToxic','Obscene','Threat','Insult','IdHate','Clean']\n",
    "# jenis = 'log'\n",
    "# model = {}\n",
    "# model2 = {}\n",
    "\n",
    "# for item in label:\n",
    "#     model['d{}_test_{}'.format(item,jenis)] = pickle.load(open('d{}_test_{}.sav'.format(item,jenis), 'rb'))\n",
    "#     model2['d{}2_test_{}'.format(item,jenis)] = pickle.load(open('d{}2_test_{}.sav'.format(item,jenis), 'rb'))\n",
    "\n",
    "# modelKey = list(model.keys())\n",
    "# model2Key = list(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
